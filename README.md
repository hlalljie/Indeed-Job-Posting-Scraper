# My-Indeed-AI
An expirement in using AI to filter job posts. 

## The current processes consists of:
### 1. Retrieve job posting data from Indeed
Through use of Selenium and Beautiful Soup, data is fetched from Indeed based on urls. Currently the data retrieved includes Job ID, Job Title, Company Name, Location, and Job Link. The inputs needed are a link to a search from Indeed and a file location for the output data. It outputs all of the data for every page of the search to a csv. This takes about a second or about 6 minutes for the 4 search links I provided, amounting to over 1000 posts. Note that other info sunch as Job pay will need to be retrieved from the Job link in a later iteration.
### 2. Cleaning and formating job data into a usable format for an LLM
Because Indeed provides multiple links to the same post, it was important to filter all of the results by unique Job ID. It was also important to insure that all of the data was being correctly read and formatted which provided some challenge.
### 3. Annototate Data for the LLM
After finding the unique IDs all of the data needed to be annotated. On the first iteration I rated only the title from 1-10 based on how likely I would be to apply to the position. My goal for the first iteration was to check the coorelation between amount of data per data point and the effectiveness of Brain.js's LLM. Currently the LLM has only used titles, but the data gathered will be compared to runs with ratings based on all of the information mentioned above. After trimming the dataset to only uniques ids, it contained 223 datapoints.
### 4. Train and Test the LLM using the Data
To train and test the model, the dataset was randomized and split into 80% (179) training points and 20%(44) test points. For each ran test the data was re-randomized and the model was retrained. Brain.js took a little over 2 hours for each training, with test time and other processes taking a negligable amount of time in comparison. From here the results were output into a CSV to be analyzed. For the first iteration 5 test were conducted on different randomized subsets of the same 223 data points.

### 5. Generating Statistics
On this first iteration the data required some cleaning as the AI model contained a small amount of junk data and output all of it's ratings as Strings. To better understand the data, a new csv was created that contained the difference between the actual rating and the Brain.js rating as well as the other data gathered before. Another csv was created for the entire test that included statistics for each sheet. This includes the Mean and Median for the real ratings, Brain.js ratings and the rating difference, the IQR, number of outliers, and Standard Devation for the real and Brain.js ratings, and the amount of correct guesses and junk data generated by the Brain.js model. 

### 6. Initial Analysis
Looking at the data, the Brain.js model tended to make safe estimates, sticking very close to the median value when choosing it's guesses. It had some success with common job titles, but overall missed more than it guessed correctly. Because it's guesses were close to the median, it failed to find outliers. Since finding outliers in order to filter bad jobs out and push good jobs to the top of my list was the goal, this iteration failed miserably. In order to improve on this model, I had planned to give the model more data per dat point (including company name, and location), but I would guess that this model will not perform any better, and might become confused if given more data per data point. I think the biggest imporvement to be made are in the amount of data, so it can see more outliers per test set, and in creating a custom model that is more specilized to finding outliers in a dataset.

### 7. Data Visualization
Data visualization in Seaborn to come, stay tuned!

## Instructions
### Job Scraping
#### 1. Choose root URLs to scrape.
On Indeed, enter custom filters to narrow down which jobs postings you would like to scrape. You can choose as many urls as you would like and the job post scraper will go through every page (up to limits that you set) so no need to include a url for every page.
#### 2. Add the urls to indeed_job_scraper.py
In the main fucntion of indeed_job_scraper.py, list the urls as strings in the url_root_list. You can replace the urls that are already there.
#### 3. Specify an Output File
Chnage the contents of the variable output_path so that it is a string that specifies the path and filename of the file to output to. This file does not need to exist, if it does, ITS CONTENTS WILL BE OVERWRITTEN.
#### 4. Specific the Limit on the Number of Posts to Scrape
Chnage the value for the variable RUN_LIMIT to the max posts you would like to scrape. This limit applies to EACH URL ROOT so if you have 4 roots with a 1000 RUN_LIMIT you can potentially scrape 4000 posts. If there are less posts for a specific root, then it will continue on to the next root after scraping all of the posts in the page.
#### 5. Run the indeed_job_scraper.py
In your favorite development enviroment, run indeed_job_scraper.py. You will need Beautiful Soup, Selenium, and Pandas installed. Note that the Selenium browser window will pop up very often and it will become the main focused window everytime this happens so it is suggested that this process is run on a virtual machine.
